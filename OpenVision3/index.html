<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenVision 3: Project Page</title>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./resources/icon.png"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
</head>
<body>

<!-- Hero -->
<section class="hero is-primary">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1">
      OpenVision 3
      <img src="resources/openvision_3_logo.png" alt="OpenVision icon" style="height: 1em; vertical-align: middle; margin-right: 0.5em;">
    </h1>
      <h2 class="subtitle is-4">A Family of Unified Visual Encoder for
        Both Understanding and Generation</h2>
      <p style="font-size: 1.15em; font-weight: 500; margin-top: 1.2em;">
        <a href="https://scholar.google.com/citations?user=o25Si3QAAAAJ&hl=en" target="_blank" style="color: white; text-decoration: underline;">Letian Zhang</a><sup>*1</sup> ·
        <a href="https://oliverrensu.github.io/" target="_blank" style="color: white; text-decoration: underline;">Sucheng Ren</a><sup>*2</sup> ·
        <a href="https://yanqing0327.github.io/Yanqing.github.io/" target="_blank" style="color: white; text-decoration: underline;">Yanqing Liu</a><sup>1</sup> ·
        <a href="https://xhl-video.github.io/xianhangli/" target="_blank" style="color: white; text-decoration: underline;">Xianhang Li</a><sup>1</sup> ·
        <a href="https://zw615.github.io/" target="_blank" style="color: white; text-decoration: underline;">Zeyu Wang</a><sup>1</sup> ·
        <a href="https://yuyinzhou.github.io/" target="_blank" style="color: white; text-decoration: underline;">Yuyin Zhou</a><sup>1</sup> ·
        <span style="color: white;">Huaxiu Yao</span><sup>3</sup> ·
        <span style="color: white;">Zeyu Zheng</span><sup>4</sup> ·
        <span style="color: white;">Weili Nie</span><sup>5</sup> ·
        <span style="color: white;">Guilin Liu</span><sup>5</sup> ·
        <span style="color: white;">Zhiding Yu</span><sup>5</sup> ·
        <a href="https://cihangxie.github.io/" target="_blank" style="color: white; text-decoration: underline;">Cihang Xie</a><sup>1</sup>
      </p>
      <p style="font-size: 1.05em; color: rgba(255, 255, 255, 0.9);">
        <sup>1</sup>UC Santa Cruz · <sup>2</sup>JHU · <sup>3</sup>UNC-Chapel Hill · <sup>4</sup>UC Berkeley · <sup>5</sup>NVIDIA
      </p>

      <!-- Buttons -->
      <div class="publication-links" style="margin-top: 1.5em;">
        <!-- PDF Link -->
        <span class="link-block">
          <a href="https://arxiv.org/abs/2505.04601"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="./resources/ar.svg" alt="arXiv" style="width: 1.2em; height: 1.2em;" />
            </span>
            <span>arXiv</span>
          </a>
        </span>

        <!-- Code Link -->
        <span class="link-block">
          <a href="https://github.com/UCSC-VLAA/OpenVision"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>Code (Coming Soon)</span>
          </a>
        </span>

        <!-- Model Link -->
        <span class="link-block">
          <a href="https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="./resources/gr.svg" alt="HF" style="width: 1.2em; height: 1.2em;" />
            </span>
            <span>Model</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image -->
<div style="max-width: 900px; margin: 2em auto; text-align: center;">
  <img src="resources/teaser_image-cropped-1.png" alt="OpenVision 3 Teaser" 
       style="max-width: 100%; height: auto; border: 0px solid #dfdede; box-shadow: 0 0px 0px rgba(0,0,0,0.1); border-radius: 0px;">
</div>

<!-- Abstract -->
<div style="max-width: 900px; margin: 2em auto; text-align: left;">
  <h2 class="title is-3 has-text-centered">Abstract</h2>
  <p style="font-size: 1.1em; line-height: 1.6;">
    This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.
  </p>
</div>


<!-- Key Contributions -->
<section class="section has-background-light">
  <div class="container">
    <h2 class="title is-3">Key Contributions</h2>
    <div class="content">
      <ul>
        <li><strong>Simple and Effective Architecture:</strong> Our tokenizer is a simple <strong>VAE + ViT</strong> encoder.</li>
        <li><strong>Unified Visual Representations:</strong> A single, unified visual representation that can serve both image understanding and image generation.</li>
        <li><strong>Superior Generation Performance:</strong> Our <strong>gFID</strong> is <strong>1.89</strong> on ImageNet, which is substantially better than CLIP+RAE(2.54) / VAE+SiT(2.06) / UniTok+LlamaGen(2.51).</li>
        <li><strong>Remarkable Reconstruction performance:</strong> OpenVision 3 achieves <strong>0.216 rFID</strong> on ImageNet 256x256.</li>
        <li><strong>Competitive Understanding Ability:</strong> Our tokenizer performs <strong>comparably with CLIP</strong> (62.4 vs 62.2 on SeedBench, 83.7 vs 82.9 on POPE).</li>
      </ul>
    </div>
  </div>
</section>


<!-- Detailed Comparisons Section -->
<section class="section has-background-light">
  <div class="container has-text-centered">
    <h2 class="title is-3">Detailed Comparisons and Efficiency</h2>

    <!-- OpenVision vs Proprietary -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Reconstruction Performance Comparison</h3>
      <img src="resources/recon.png" alt="OpenVision vs Proprietary Encoders"
           style="max-width: 60%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision 3 outperforms existing unified tokenizers across all metrics on ImageNet 256x256. 
        <br>
        Even in comparison with specialized generation-oriented tokenizers, our model maintains competitive or better results.
      </p>
    </div>

    
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Class-conditional Image Generation Performance</h3>
      <img src="resources/gen_table.png" alt="OpenVision vs Proprietary Encoders"
           style="max-width: 50%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision 3 achieves higher generation fidelity under RAE framework on ImageNet than SD-VAE+SiT, UniTok+LlamaGen, CLIP+RAE and OpenVision+RAE.
      </p>
    </div>

    <!-- LLaVA-1.5 Performance -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Performance under LLaVA-1.5 Framework</h3>
      <img src="resources/und_table.png" alt="LLaVA-1.5 Performance Comparison"
           style="max-width: 80%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision demonstrates comparible performance with CLIP on multiple multimodal benchmarks wich much less computational cost.
      </p>
    </div>

    <!-- Open-LLaVA-Next Performance
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Performance under Open-LLaVA-Next Framework</h3>
      <img src="resources/openvision_llava_next.jpg" alt="Open-LLaVA-Next Performance Comparison"
           style="max-width: 70%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        Under Open-LLaVA-Next, OpenVision maintains its competitive edge, excelling particularly in document-heavy multimodal tasks.
      </p>
    </div>

    <!-- Efficiency Comparison -->
    <!--
    <div class="content">
      <h3 class="title is-4">Efficiency Comparison</h3>
      <img src="resources/efficiency_llava1.5_vs_next_resolution_size.png" alt="Efficiency Comparison"
           style="max-width: 50%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision achieves superior multimodal performance with significantly reduced training time compared to proprietary alternatives.
      </p>
    </div> -->

  </div>
</section>


<!-- Model Zoo (ImageNet-1K) -->
<section class="section">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Model Zoo (ImageNet-1K)</h2>
    <p class="has-text-centered" style="max-width: 700px; margin: 0 auto 1.5em auto; font-size: 1.05em;">
      We report ImageNet-1K Top-1 accuracy across OpenVision variants. All models are available in both JAX and PyTorch formats.
    </p>
    <div class="table-container" style="overflow-x: auto; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 10px;">
      <table class="table is-bordered is-fullwidth has-text-centered" style="min-width: 900px;">
        <thead class="has-background-light">
          <tr>
            <th>Model</th><th>Size</th><th>Patch</th><th>Resolution</th><th>Top-1</th><th>Link</th><th>JAX</th><th>PyTorch</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>ViT-Tiny</td><td>5M</td><td>16</td><td>160</td><td>46.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch16-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>16</td><td>224</td><td>49.6%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch16-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>16</td><td>384</td><td>51.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch16-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>8</td><td>160</td><td>51.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch8-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>8</td><td>224</td><td>53.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch8-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>8</td><td>384</td><td>53.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch8-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>

          <tr><td>ViT-Small</td><td>22M</td><td>16</td><td>160</td><td>63.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch16-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>16</td><td>224</td><td>65.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch16-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>16</td><td>384</td><td>67.1%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch16-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>8</td><td>160</td><td>67.3%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch8-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>8</td><td>224</td><td>68.6%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch8-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>8</td><td>384</td><td>68.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch8-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>

          <tr><td>ViT-Base</td><td>86M</td><td>16</td><td>160</td><td>72.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch16-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>16</td><td>224</td><td>73.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch16-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>16</td><td>384</td><td>74.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch16-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>8</td><td>160</td><td>74.8%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch8-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>8</td><td>224</td><td>75.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch8-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>8</td><td>384</td><td>75.6%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch8-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>

          <tr><td>ViT-Large</td><td>307M</td><td>14</td><td>84</td><td>74.7%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-large-patch14-84" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Large</td><td>307M</td><td>14</td><td>224</td><td>78.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-large-patch14-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Large</td><td>307M</td><td>14</td><td>336</td><td>78.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-large-patch14-336" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>SoViT-400M</td><td>400M</td><td>14</td><td>84</td><td>76.2%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-so400m-patch14-84" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>SoViT-400M</td><td>400M</td><td>14</td><td>224</td><td>79.7%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-so400m-patch14-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>SoViT-400M</td><td>400M</td><td>14</td><td>384</td><td>79.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-so400m-patch14-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Huge</td><td>632M</td><td>14</td><td>84</td><td>77.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-huge-patch14-84" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Huge</td><td>632M</td><td>14</td><td>224</td><td>80.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-huge-patch14-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<!-- Model Usage Section -->
<section class="section">
  <div class="container content">
    <h2 class="title is-3">Model Usage</h2>
    <h3 class="subtitle is-5">With Our Customized OpenCLIP Tokenizer</h3>

    <p>
      ⚠️ <strong>IMPORTANT:</strong> Make sure you're importing from <code>src/convert_upload/open_clip/</code> in this repo.
      <br>
      The tokenizer implementation here is <strong>customized</strong> and not yet available in the official OpenCLIP repo or PyPI release.
    </p>

    <pre><code class="language-python">
import torch
import torch.nn.functional as F
from urllib.request import urlopen
from PIL import Image

# ⚠️ Use our repo's tokenizer implementation at src/convert_upload/open_clip/
from open_clip import create_model_from_pretrained, get_tokenizer

model, preprocess = create_model_from_pretrained('hf-hub:UCSC-VLAA/openvision-vit-large-patch14-224')
tokenizer = get_tokenizer('hf-hub:UCSC-VLAA/openvision-vit-large-patch14-224')

image = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))
image = preprocess(image).unsqueeze(0)

text = tokenizer(["a diagram", "a dog", "a cat", "a beignet"], context_length=model.context_length)

with torch.no_grad(), torch.cuda.amp.autocast():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    image_features = F.normalize(image_features, dim=-1)
    text_features = F.normalize(text_features, dim=-1)

    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)

print("Label probs:", text_probs)  # prints: [[0., 0., 0., 1.0]]
    </code></pre>
  </div>
</section>

<!-- Acknowledgement -->
<section class="section">
  <div class="container has-text-centered">
    <h2 class="title is-3">Acknowledgement</h2>
    <p style="font-size: 1em; color: #555;">
      We would like to thank the <strong>TPU Research Cloud (TRC)</strong> program and <strong>Google Cloud Research Credits</strong> program for supporting our computing needs.
    </p>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{li2025openvision,
  title   = {OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning},
  author  = {Li, Xianhang and Liu, Yanqing and Tu, Haoqin and Zhu, Hongru and Xie, Cihang},
  journal = {arXiv preprint arXiv:2505.04601},
  year    = {2025}
}
    </code></pre>
  </div>
</section>


<!-- Footer
<footer class="footer">
  <div class="content has-text-centered">
      <p>
      Built by the UCSC-VLAA team.
      <a href="https://scholar.google.com/citations?user=o25Si3QAAAAJ&hl=en" target="_blank">Letian Zhang</a>,
      <a href="https://oliverrensu.github.io/" target="_blank">Sucheng Ren</a>,
      <a href="https://yanqing0327.github.io/Yanqing.github.io/" target="_blank">Yanqing Liu</a>,
      <a href="https://xhl-video.github.io/xianhangli/" target="_blank">Xianhang Li</a>,
      <a href="https://zw615.github.io/" target="_blank">Zeyu Wang</a>,
      <a href="https://yuyinzhou.github.io/" target="_blank">Yuyin Zhou</a>,
      <a href="https://cihangxie.github.io/" target="_blank">Cihang Xie</a>.
    </p>
  </div>
</footer> -->


</body>
</html>

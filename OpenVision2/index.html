<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>OpenVision 2</title>
  <meta name="description" content="OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning" />
  <link rel="icon" href="resources/openvision_v1.5_logo.png" />
  <style>
    :root{
      --bg:#0b0e14; --panel:#0f1320; --ink:#e8ecf1; --muted:#9aa7b1;
      --brand:#7cd1ff; --brand2:#ffd166; --ok:#7ee787; --warn:#ffd580;
      --code:#0d1117; --border:#283044; --table:#12182a; --table-alt:#0f1526;
      --accent:#60a5fa;
    }
    *{box-sizing:border-box}
    html,body{margin:0;padding:0;background:var(--bg);color:var(--ink);font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,"Helvetica Neue",Arial}
    a{color:var(--brand);text-decoration:none}
    a:hover{opacity:.9;text-decoration:underline}
    code,kbd,pre{font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,"Liberation Mono",monospace}
    .container{max-width:1100px;margin:0 auto;padding:28px}
    header{position:sticky;top:0;z-index:20;background:rgba(11,14,20,.7);backdrop-filter:blur(10px);border-bottom:1px solid var(--border)}
    .nav{display:flex;gap:18px;flex-wrap:wrap;align-items:center;justify-content:space-between}
    .nav .links{display:flex;gap:16px;flex-wrap:wrap}
    .brand{display:flex;gap:10px;align-items:center}
    .brand img.logo{width:30px;height:18px;object-fit:cover;border-radius:3px; /* 扁一点 */}
    .hero{display:grid;grid-template-columns:1fr;gap:18px;align-items:center;padding:28px 0}
    .badge{display:inline-flex;gap:.5ch;align-items:center;background:linear-gradient(90deg,#1b2238,#141b2f);border:1px solid var(--border);border-radius:999px;padding:6px 12px;color:var(--muted);font-weight:600}
    h1{font-size:40px;line-height:1.15;margin:.25em 0 .4em}
    h2{font-size:28px;margin:1.3em 0 .5em}
    h3{font-size:22px;margin:1.1em 0 .4em}
    .panel{background:var(--panel);border:1px solid var(--border);border-radius:14px;padding:18px}
    .grid{display:grid;gap:18px}
    .grid.cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}
    .muted{color:var(--muted)}
    .kpis{display:grid;gap:12px;grid-template-columns:repeat(2,minmax(0,1fr))}
    .kpi{background:linear-gradient(180deg,#101831,#0d1428);border:1px solid var(--border);border-radius:12px;padding:14px}
    .kpi strong{display:block;font-size:20px;color:var(--ok)}
    pre{background:var(--code);border:1px solid var(--border);border-radius:12px;padding:16px;overflow:auto}
    table{width:100%;border-collapse:separate;border-spacing:0;overflow:auto;background:var(--table);border:1px solid var(--border);border-radius:12px}
    th,td{padding:10px 12px;border-bottom:1px solid var(--border);text-align:center;font-size:14px}
    thead th{position:sticky;top:0;background:var(--table-alt);text-transform:uppercase;letter-spacing:.04em;font-weight:700}
    tbody tr:nth-child(odd){background:rgba(255,255,255,.02)}
    .good{color:var(--ok);font-weight:700}
    .foot{color:var(--muted);font-size:14px}
    .hr{height:1px;background:var(--border);margin:24px 0}

    /* Full-bleed image section */
    .bleed {margin:0;padding:0}
    .bleed img {
      display: block;
      width: 70vw;
      max-width: 70vw;
      margin: 0 auto;   /* 自动左右居中 */
      border: none;
      border-radius: 0;
    }

    @media (max-width:960px){
      .grid.cols-2{grid-template-columns:1fr}
    }
  </style>
</head>
<body>

<header>
  <div class="container nav">
    <div class="brand">
      <img class="logo" src="resources/openvision_v1.5_logo.png" alt="OV" />
      <strong>OpenVision</strong>
    </div>
    <div class="links">
      <a href="#overview">Overview</a>
      <a href="#whats-new">What’s New</a>
      <a href="#benchmarks">Benchmarks</a>
      <a href="#efficiency">Efficiency</a>
      <a href="#vision-only">Vision-only Loading</a>
      <a href="#install">Install</a>
      <a href="#cite">Cite</a>
    </div>
  </div>
</header>

<main class="container">
  <!-- HERO (text only now) -->
  <section class="hero" id="overview">
    <div>
      <span class="badge">Fully-Open Vision Encoders • Generative Pretraining</span>
      <h1>OpenVision&nbsp;2</h1>
      <p class="muted">
        <strong>OpenVision</strong>: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning.<br/>
        <strong>OpenVision&nbsp;2</strong>: A Family of <em>Generative Pretrained</em> Visual Encoders that removes the text encoder and contrastive loss, training with caption-only supervision.
      </p>

      <div class="grid cols-2" style="margin-top:12px">
        <div class="panel">
          <h3 style="margin-top:0">Quick Links (OpenVision)</h3>
          <ul style="margin:8px 0 0 18px">
            <li>Project page: <a target="_blank" href="https://ucsc-vlaa.github.io/OpenVision/">OpenVision</a></li>
            <li>ArXiv: <a target="_blank" href="https://arxiv.org/abs/2505.04601">arXiv:2505.04601</a></li>
            <li>Code: <a target="_blank" href="https://github.com/UCSC-VLAA/OpenVision">GitHub</a></li>
            <li>HF Collection: <a target="_blank" href="https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919">OpenVision Collection</a></li>
          </ul>
        </div>
        <div class="panel">
          <h3 style="margin-top:0">Quick Links (OpenVision&nbsp;2)</h3>
          <ul style="margin:8px 0 0 18px">
            <li>Project page: <a target="_blank" href="https://ucsc-vlaa.github.io/OpenVision2/">OpenVision 2</a></li>
            <li>ArXiv: <a target="_blank" href="https://arxiv.org/abs/2509.xxxxx">arXiv:2509.xxxxx</a></li>
            <li>HF Collection: <a target="_blank" href="https://huggingface.co/collections/UCSC-VLAA/openvision-2-68ab5934fe21f3fc463077da">OpenVision 2 Collection</a></li>
            <li>Dataset: <a target="_blank" href="https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B">ReCap-DataComp-1B v2</a></li>
          </ul>
        </div>
      </div>

      <div class="kpis" style="margin-top:14px">
        <div class="kpi"><span class="muted">Training speed</span><strong>1.5–2× faster</strong></div>
        <div class="kpi"><span class="muted">Memory footprint</span><strong>~1.8× lower</strong></div>
        <div class="kpi"><span class="muted">Scale</span><strong>Up to 1B+ params</strong></div>
        <div class="kpi"><span class="muted">Benchmarks</span><strong>OCR/TextVQA↑</strong></div>
      </div>
    </div>
  </section>
</main>

<!-- FULL-BLEED TEASER (entire screen width) -->
<section class="bleed" aria-label="OpenVision 2 teaser">
  <img src="resources/openvision2_teaser.png" alt="OpenVision 2 teaser, full width" />
</section>

<main class="container">
  <div class="hr"></div>

  <!-- WHAT'S NEW -->
  <section id="whats-new" class="grid">
    <h2>What’s New in <span style="color:var(--brand2)">OpenVision&nbsp;2</span></h2>
    <div class="panel">
      <ul style="margin:0 0 0 18px">
        <li><strong>Caption-only generative training</strong>: ViT vision encoder + decoder-only text model. No text encoder, no contrastive loss.</li>
        <li><strong>Training–inference alignment</strong>: Pretraining pipeline mirrors modern MLLM usage (e.g., LLaVA), reducing objective mismatch.</li>
        <li><strong>Efficiency at scale</strong>: CLIPA two-stage curriculum (low-res pretraining → short high-res finetune) + <em>visual token masking</em> (keep ~25–35% tokens).</li>
        <li><strong>High-quality captions</strong>: ReCap-DataComp-1B v2 (LLaMA-3 powered, conditioned on alt-text with weighted top-k sampling) for richer, grounded supervision.</li>
      </ul>
    </div>
  </section>

  <div class="hr"></div>

  <!-- BENCHMARKS -->
  <section id="benchmarks">
    <h2>Benchmarks (OpenVision vs OpenVision&nbsp;2)</h2>
    <p class="muted">We report results under two MLLM frameworks. <strong>MME</strong> is shown as Perception/Cognition.</p>

    <h3>LLaVA-1.5</h3>
    <div class="panel">
      <div style="overflow:auto">
        <table>
          <thead>
            <tr>
              <th>Method</th><th>Vision Encoder</th><th>Params</th><th>Res</th>
              <th>TextVQA</th><th>ChartQA</th><th>OCR</th><th>MME (P/K)</th>
              <th>SEED</th><th>SQA</th><th>GQA</th><th>POPE</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>OpenVision</td><td>L/14</td><td>304M</td><td>224</td><td>57.7</td><td>13.9</td><td>315</td><td>1487/317</td><td>69.5</td><td>73.6</td><td>62.9</td><td>86.4</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>L/14</strong></td><td><strong>304M</strong></td><td><strong>224</strong></td><td class="good"><strong>59.0</strong></td><td><strong>13.7</strong></td><td class="good"><strong>327</strong></td><td><strong>1460/312</strong></td><td><strong>69.3</strong></td><td><strong>76.5</strong></td><td><strong>62.6</strong></td><td><strong>87.1</strong></td></tr>
            <tr><td>OpenVision</td><td>L/14</td><td>304M</td><td>336</td><td>61.2</td><td>15.7</td><td>339</td><td>1525/315</td><td>70.5</td><td>75.1</td><td>63.7</td><td>87.2</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>L/14</strong></td><td><strong>304M</strong></td><td><strong>336</strong></td><td class="good"><strong>63.0</strong></td><td><strong>14.5</strong></td><td class="good"><strong>357</strong></td><td><strong>1486/321</strong></td><td><strong>70.1</strong></td><td><strong>77.5</strong></td><td><strong>63.0</strong></td><td><strong>87.7</strong></td></tr>
            <tr><td>OpenVision</td><td>SoViT-400M/14</td><td>400M</td><td>384</td><td>62.4</td><td>16.1</td><td>357</td><td>1493/320</td><td>70.4</td><td>72.4</td><td>63.8</td><td>88.0</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>SoViT-400M/14</strong></td><td><strong>400M</strong></td><td><strong>384</strong></td><td class="good"><strong>64.3</strong></td><td><strong>15.0</strong></td><td class="good"><strong>387</strong></td><td><strong>1472/310</strong></td><td><strong>70.7</strong></td><td><strong>74.9</strong></td><td><strong>63.5</strong></td><td><strong>87.5</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>H/14</strong></td><td><strong>632M</strong></td><td><strong>224</strong></td><td><strong>60.2</strong></td><td><strong>13.5</strong></td><td><strong>340</strong></td><td><strong>1470/305</strong></td><td><strong>69.3</strong></td><td><strong>75.4</strong></td><td><strong>62.5</strong></td><td><strong>87.2</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>H/14</strong></td><td><strong>632M</strong></td><td><strong>336</strong></td><td><strong>63.4</strong></td><td><strong>16.3</strong></td><td><strong>391</strong></td><td><strong>1470/311</strong></td><td><strong>70.6</strong></td><td><strong>76.4</strong></td><td><strong>63.1</strong></td><td><strong>88.4</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>H/14</strong></td><td><strong>632M</strong></td><td><strong>448</strong></td><td><strong>65.6</strong></td><td><strong>18.1</strong></td><td><strong>416</strong></td><td><strong>1499/331</strong></td><td><strong>70.6</strong></td><td><strong>75.6</strong></td><td><strong>63.1</strong></td><td><strong>88.7</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>g/14</strong></td><td><strong>1.01B</strong></td><td><strong>224</strong></td><td><strong>60.2</strong></td><td><strong>13.7</strong></td><td><strong>338</strong></td><td><strong>1469/290</strong></td><td><strong>69.3</strong></td><td><strong>75.0</strong></td><td><strong>62.6</strong></td><td><strong>86.9</strong></td></tr>
          </tbody>
        </table>
      </div>
    </div>

    <h3 style="margin-top:22px">Open-LLaVA-Next</h3>
    <div class="panel">
      <div style="overflow:auto">
        <table>
          <thead>
            <tr>
              <th>Method</th><th>Vision Encoder</th><th>Params</th><th>Res</th>
              <th>TextVQA</th><th>ChartQA</th><th>OCR</th><th>MME (P/K)</th>
              <th>SEED</th><th>SQA</th><th>GQA</th><th>POPE</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>OpenVision</td><td>L/14</td><td>304M</td><td>224</td><td>65.7</td><td>61.5</td><td>503</td><td>1567/332</td><td>73.1</td><td>73.1</td><td>64.7</td><td>87.8</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>L/14</strong></td><td><strong>304M</strong></td><td><strong>224</strong></td><td class="good"><strong>66.1</strong></td><td><strong>60.4</strong></td><td><strong>501</strong></td><td><strong>1577/297</strong></td><td><strong>73.1</strong></td><td><strong>68.4</strong></td><td><strong>64.6</strong></td><td><strong>87.6</strong></td></tr>
            <tr><td>OpenVision</td><td>L/14</td><td>304M</td><td>336</td><td>68.3</td><td>68.0</td><td>547</td><td>1520/310</td><td>73.3</td><td>75.4</td><td>64.4</td><td>88.1</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>L/14</strong></td><td><strong>304M</strong></td><td><strong>336</strong></td><td class="good"><strong>68.9</strong></td><td><strong>62.3</strong></td><td><strong>537</strong></td><td><strong>1585/278</strong></td><td><strong>73.4</strong></td><td><strong>75.2</strong></td><td><strong>64.6</strong></td><td><strong>88.4</strong></td></tr>
            <tr><td>OpenVision</td><td>SoViT-400M/14</td><td>400M</td><td>384</td><td>67.4</td><td>63.1</td><td>540</td><td>1500/353</td><td>72.2</td><td>73.5</td><td>63.4</td><td>87.8</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>SoViT-400M/14</strong></td><td><strong>400M</strong></td><td><strong>384</strong></td><td class="good"><strong>69.0</strong></td><td><strong>63.4</strong></td><td class="good"><strong>549</strong></td><td><strong>1521/319</strong></td><td><strong>72.2</strong></td><td><strong>72.7</strong></td><td><strong>63.1</strong></td><td><strong>87.7</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>H/14</strong></td><td><strong>632M</strong></td><td><strong>224</strong></td><td><strong>66.4</strong></td><td><strong>60.2</strong></td><td><strong>514</strong></td><td><strong>1597/314</strong></td><td><strong>73.3</strong></td><td><strong>76.2</strong></td><td><strong>64.7</strong></td><td><strong>88.4</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>H/14</strong></td><td><strong>632M</strong></td><td><strong>336</strong></td><td><strong>69.9</strong></td><td><strong>64.8</strong></td><td><strong>573</strong></td><td><strong>1572/337</strong></td><td><strong>73.8</strong></td><td><strong>74.5</strong></td><td><strong>64.4</strong></td><td><strong>87.8</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>H/14</strong></td><td><strong>632M</strong></td><td><strong>448</strong></td><td><strong>71.9</strong></td><td><strong>64.9</strong></td><td><strong>590</strong></td><td><strong>1542/324</strong></td><td><strong>74.1</strong></td><td><strong>75.6</strong></td><td><strong>64.4</strong></td><td><strong>88.8</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>g/14</strong></td><td><strong>1.01B</strong></td><td><strong>224</strong></td><td><strong>67.3</strong></td><td><strong>62.4</strong></td><td><strong>514</strong></td><td><strong>1558/323</strong></td><td><strong>73.4</strong></td><td><strong>74.4</strong></td><td><strong>64.7</strong></td><td><strong>88.0</strong></td></tr>
          </tbody>
        </table>
      </div>
    </div>
  </section>

  <div class="hr"></div>

  <!-- EFFICIENCY -->
  <section id="efficiency">
    <h2>Efficiency &amp; Scaling</h2>

    <div class="grid cols-2">
      <div class="panel">
        <h3 style="margin-top:0">Training (TPU v4-512)</h3>
        <table>
          <thead><tr><th>Model</th><th>Backbone</th><th>Res</th><th>v4-512 Hours</th><th>FLOPs / Image</th></tr></thead>
          <tbody>
            <tr><td>OpenVision</td><td>L/14</td><td>224</td><td>83</td><td>271.75</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>L/14</strong></td><td><strong>224</strong></td><td class="good"><strong>57</strong></td><td class="good"><strong>208.90</strong></td></tr>
            <tr><td>OpenVision</td><td>SoViT-400M/14</td><td>384</td><td>241</td><td>1636.75</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>SoViT-400M/14</strong></td><td><strong>384</strong></td><td class="good"><strong>121</strong></td><td class="good"><strong>1017.74</strong></td></tr>
          </tbody>
        </table>
      </div>

      <div class="panel">
        <h3 style="margin-top:0">Memory (TPU v4-64, GB/chip)</h3>
        <table>
          <thead><tr><th>Model (L/14)</th><th>Res</th><th>Batch</th><th>Peak Mem</th></tr></thead>
          <tbody>
            <tr><td>OpenVision</td><td>224</td><td>2k</td><td>24.5</td></tr>
            <tr><td>OpenVision</td><td>224</td><td>4k</td><td>OOM</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>224</strong></td><td><strong>2k</strong></td><td class="good"><strong>13.8</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>224</strong></td><td><strong>4k</strong></td><td class="good"><strong>22.1</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>224</strong></td><td><strong>8k</strong></td><td class="good"><strong>28.4</strong></td></tr>
          </tbody>
        </table>
        <div style="height:10px"></div>
        <table>
          <thead><tr><th>Model (SoViT-400M/14)</th><th>Res</th><th>Batch</th><th>Peak Mem</th></tr></thead>
          <tbody>
            <tr><td>OpenVision</td><td>384</td><td>512</td><td>27.4</td></tr>
            <tr><td>OpenVision</td><td>384</td><td>1k</td><td>OOM</td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>384</strong></td><td><strong>512</strong></td><td class="good"><strong>14.5</strong></td></tr>
            <tr><td><strong>OpenVision 2</strong></td><td><strong>384</strong></td><td><strong>1k</strong></td><td class="good"><strong>28.8</strong></td></tr>
          </tbody>
        </table>
      </div>
    </div>

    <div class="panel" style="margin-top:18px">
      <h3 style="margin-top:0">Strategy Ablations (ViT-L/14 @ 224, v4-64)</h3>
      <table>
        <thead><tr><th>Method</th><th>CLIPA</th><th>Token Mask</th><th>Time (h)</th></tr></thead>
        <tbody>
          <tr><td>CapPa baseline</td><td>–</td><td>–</td><td>217</td></tr>
          <tr><td>OV2 (Mask only)</td><td>–</td><td>✓</td><td>190</td></tr>
          <tr><td>OV2 (CLIPA only)</td><td>✓</td><td>–</td><td>67</td></tr>
          <tr><td><strong>OV2 (both)</strong></td><td><strong>✓</strong></td><td><strong>✓</strong></td><td class="good"><strong>55</strong></td></tr>
        </tbody>
      </table>
    </div>
  </section>

  <div class="hr"></div>

  <!-- VISION-ONLY LOADING -->
  <section id="vision-only">
    <h2>How to Load the Converted <em>Vision-Only</em> Encoder</h2>
    <p class="muted">Use our OpenCLIP-compatible interface to load vision-only checkpoints from the Hugging Face Hub.</p>
    <pre><code>import torch
from open_clip.factory import create_vision_encoder_and_transforms

# Replace with your HF repo containing the converted vision-only weights
hf_repo = "UCSC-VLAA/openvision2-vit-large-patch14-224-vision-only"

vision_encoder = create_vision_encoder_and_transforms(
    model_name=f"hf-hub:{hf_repo}"
)
vision_encoder.eval()

dummy = torch.ones(1, 3, 224, 224)
with torch.no_grad():
    pooled, tokens = vision_encoder(dummy)
print("pooled:", tuple(pooled.shape), "tokens:", tuple(tokens.shape))</code></pre>
  </section>

  <div class="hr"></div>

  <!-- INSTALLATION -->
  <section id="install">
    <h2>Installation</h2>
    <pre><code># Clone
git clone https://github.com/UCSC-VLAA/OpenVision.git
cd OpenVision

# TPU-oriented environment (example)
bash setup.sh DEVICE=tpu JAX_VERSION=0.4.38

# Optional (PyTorch-side use)
pip install open_clip_torch huggingface_hub</code></pre>

    <h3>Data &amp; Training (Overview)</h3>
    <ul class="muted">
      <li>Pretraining data: <strong>ReCap-DataComp-1B v2</strong> (synthetic, conditioned on alt-text)</li>
      <li>CLIPA schedule: low-res pretraining → short high-res finetune</li>
      <li>Visual token masking: keep ~25–35% of tokens into the decoder (reduces compute &amp; regularizes)</li>
      <li>Scripts: <code>scripts/project/openvision/train.sh</code> (TPU Pod helper <code>tpu_command.sh</code>)</li>
    </ul>
  </section>

  <div class="hr"></div>

  <!-- CITATION -->
  <section id="cite">
    <h2>Citation</h2>
    <pre><code>@article{li2025openvision,
  title   = {OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning},
  author  = {Li, Xianhang and Liu, Yanqing and Tu, Haoqin and Zhu, Hongru and Xie, Cihang},
  journal = {arXiv preprint arXiv:2505.04601},
  year    = {2025}
}</code></pre>

    <pre><code>@article{liu2025openvision2,
  title   = {OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning},
  author  = {Liu, Yanqing and Li, Xianhang and Zhang, Letian and Wang, Zirui and Zheng, Zeyu and Zhou, Yuyin and Xie, Cihang},
  journal = {arXiv preprint arXiv:2509.xxxxx},
  year    = {2025}
}</code></pre>

    <p class="foot">License: Apache-2.0. JAX implementation builds on Big Vision; PyTorch side builds on OpenCLIP, with references to timm and MAE. Thanks to TPU Research Cloud (TRC) and Google Cloud Research Credits for support.</p>
  </section>

  <div style="height:40px"></div>
</main>

</body>
</html>

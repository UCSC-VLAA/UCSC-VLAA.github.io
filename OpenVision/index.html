<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenVision: Project Page</title>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./resources/icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
</head>
<body>

<!-- Hero -->
<section class="hero is-primary">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1">OpenVision</h1>
      <h2 class="subtitle is-4">Fully-Open, Cost-Effective Vision Encoders for Multimodal Learning</h2>
      <p style="font-size: 1.15em; font-weight: 500; margin-top: 1.2em;">
        <a href="https://xhl-video.github.io/xianhangli/" target="_blank" style="color: white; text-decoration: underline;">Xianhang Li</a><sup>*</sup> ·
        <a href="https://yanqing0327.github.io/Yanqing.github.io/" target="_blank" style="color: white; text-decoration: underline;">Yanqing Liu</a><sup>*</sup> ·
        <a href="https://www.haqtu.me/" target="_blank" style="color: white; text-decoration: underline;">Haoqin Tu</a> ·
        <a href="https://scholar.google.com/citations?user=G8NZJLIAAAAJ&hl=en" target="_blank" style="color: white; text-decoration: underline;">Hongru Zhu</a> ·
        <a href="https://cihangxie.github.io/" target="_blank" style="color: white; text-decoration: underline;">Cihang Xie</a>
      </p>
      <p style="font-size: 1.05em; color: rgba(255, 255, 255, 0.9);">
        University of California, Santa Cruz
      </p>

      <!-- Buttons -->
      <div class="publication-links" style="margin-top: 1.5em;">
        <!-- PDF Link -->
        <span class="link-block">
          <a href="https://arxiv.org/abs/2406.08478"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="./resources/ar.svg" alt="arXiv" style="width: 1.2em; height: 1.2em;" />
            </span>
            <span>arXiv</span>
          </a>
        </span>

        <!-- Code Link -->
        <span class="link-block">
          <a href="https://github.com/UCSC-VLAA/OpenVision"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>Code</span>
          </a>
        </span>

        <!-- Model Link -->
        <span class="link-block">
          <a href="https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="./resources/gr.svg" alt="HF" style="width: 1.2em; height: 1.2em;" />
            </span>
            <span>Model</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<div style="max-width: 900px; margin: 2em auto; text-align: left;">
  <h2 class="title is-3 has-text-centered">Abstract</h2>
  <p style="font-size: 1.1em; line-height: 1.6;">
    OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released.
This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI’s CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works---e.g., CLIPS for training framework and Recap-DataComp-1B for training data---while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models.  By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.
  </p>
</div>


<!-- Key Contributions -->
<section class="section has-background-light">
  <div class="container">
    <h2 class="title is-3">Key Contributions</h2>
    <div class="content">
      <ul>
        <li><strong>Fully Open Vision Encoders:</strong> Datasets, training recipes, and model checkpoints are entirely public, fostering reproducibility and transparency in multimodal research.</li>
        <li><strong>Wide Range of Model Scales:</strong> A comprehensive family of vision encoders from Tiny (5.9M) to Huge (632.1M) parameters, supporting deployment from edge devices to high-capacity servers.</li>
        <li><strong>Superior Multimodal Performance:</strong> Matches or surpasses proprietary vision encoders (e.g., OpenAI-CLIP, SigLIP) across popular multimodal benchmarks (e.g., LLaVA-1.5, Open-LLaVA-Next).</li>
        <li><strong>Efficient Progressive Resolution Training:</strong> Demonstrates significant efficiency improvements (2×–3× faster) compared to proprietary counterparts through a progressive, multi-stage resolution training strategy.</li>
        <li><strong>Flexible Patch-Size Configuration:</strong> Supports adaptive encoding (8×8, 16×16 patches), allowing detailed visual understanding or efficient processing based on practical needs.</li>
      </ul>
    </div>
  </div>
</section>


<!-- Detailed Comparisons Section -->
<section class="section has-background-light">
  <div class="container has-text-centered">
    <h2 class="title is-3">Detailed Comparisons and Efficiency</h2>

    <!-- OpenVision vs Proprietary -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">OpenVision vs. Proprietary Encoders</h3>
      <img src="resources/openvision_teaser_v1.3.png" alt="OpenVision vs Proprietary Encoders"
           style="max-width: 60%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision encoders match or outperform proprietary models like OpenAI's CLIP and Google's SigLIP across multimodal tasks.
      </p>
    </div>

    <!-- LLaVA-1.5 Performance -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Performance under LLaVA-1.5 Framework</h3>
      <img src="resources/performance_normalized.png" alt="LLaVA-1.5 Performance Comparison"
           style="max-width: 50%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision demonstrates strong performance improvements over existing CLIP models under the LLaVA-1.5 multimodal framework.
      </p>
    </div>

    <!-- Open-LLaVA-Next Performance -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Performance under Open-LLaVA-Next Framework</h3>
      <img src="resources/performance_normalized_2.png" alt="Open-LLaVA-Next Performance Comparison"
           style="max-width: 50%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        Under Open-LLaVA-Next, OpenVision maintains its competitive edge, excelling particularly in document-heavy multimodal tasks.
      </p>
    </div>

    <!-- Efficiency Comparison -->
    <div class="content">
      <h3 class="title is-4">Efficiency Comparison</h3>
      <img src="resources/efficiency_llava1.5_vs_next_resolution_size.png" alt="Efficiency Comparison"
           style="max-width: 50%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision achieves superior multimodal performance with significantly reduced training time compared to proprietary alternatives.
      </p>
    </div>

  </div>
</section>


<!-- Model Zoo (ImageNet-1K) -->
<section class="section">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Model Zoo (ImageNet-1K)</h2>
    <p class="has-text-centered" style="max-width: 700px; margin: 0 auto 1.5em auto; font-size: 1.05em;">
      We report ImageNet-1K Top-1 accuracy across OpenVision variants. All models are available in both JAX and PyTorch formats.
    </p>
    <div class="table-container" style="overflow-x: auto; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 10px;">
      <table class="table is-bordered is-fullwidth has-text-centered" style="min-width: 900px;">
        <thead class="has-background-light">
          <tr>
            <th>Model</th><th>Size</th><th>Patch</th><th>Resolution</th><th>Top-1</th><th>Link</th><th>JAX</th><th>PyTorch</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>ViT-Tiny</td><td>5M</td><td>16</td><td>160</td><td>46.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch16-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>16</td><td>224</td><td>49.6%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch16-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>16</td><td>384</td><td>51.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch16-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>8</td><td>160</td><td>51.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch8-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>8</td><td>224</td><td>53.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch8-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Tiny</td><td>5M</td><td>8</td><td>384</td><td>53.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-tiny-patch8-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>

          <tr><td>ViT-Small</td><td>22M</td><td>16</td><td>160</td><td>63.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch16-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>16</td><td>224</td><td>65.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch16-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>16</td><td>384</td><td>67.1%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch16-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>8</td><td>160</td><td>67.3%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch8-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>8</td><td>224</td><td>68.6%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch8-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Small</td><td>22M</td><td>8</td><td>384</td><td>68.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-small-patch8-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>

          <tr><td>ViT-Base</td><td>86M</td><td>16</td><td>160</td><td>72.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch16-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>16</td><td>224</td><td>73.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch16-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>16</td><td>384</td><td>74.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch16-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>8</td><td>160</td><td>74.8%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch8-160" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>8</td><td>224</td><td>75.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch8-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Base</td><td>86M</td><td>8</td><td>384</td><td>75.6%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-base-patch8-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>

          <tr><td>ViT-Large</td><td>307M</td><td>14</td><td>84</td><td>74.7%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-large-patch14-84" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Large</td><td>307M</td><td>14</td><td>224</td><td>78.5%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-large-patch14-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Large</td><td>307M</td><td>14</td><td>336</td><td>78.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-large-patch14-336" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>SoViT-400M</td><td>400M</td><td>14</td><td>84</td><td>76.2%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-so400m-patch14-84" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>SoViT-400M</td><td>400M</td><td>14</td><td>224</td><td>79.7%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-so400m-patch14-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>SoViT-400M</td><td>400M</td><td>14</td><td>384</td><td>79.9%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-so400m-patch14-384" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Huge</td><td>632M</td><td>14</td><td>84</td><td>77.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-huge-patch14-84" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
          <tr><td>ViT-Huge</td><td>632M</td><td>14</td><td>224</td><td>80.4%</td><td><a href="https://huggingface.co/UCSC-VLAA/openvision-vit-huge-patch14-224" target="_blank">HF</a></td><td>✓</td><td>✓</td></tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<!-- Acknowledgement -->
<section class="section">
  <div class="container has-text-centered">
    <h2 class="title is-3">Acknowledgement</h2>
    <p style="font-size: 1em; color: #555;">
      We would like to thank the <strong>TPU Research Cloud (TRC)</strong> program and <strong>Google Cloud Research Credits</strong> program for supporting our computing needs.
    </p>
  </div>
</section>


<!-- Footer -->
<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Built by the UCSC-VLAA team.
      <a href="https://xhl-video.github.io/xianhangli/" target="_blank">Xianhang Li</a>,
      <a href="https://yanqing0327.github.io/Yanqing.github.io/" target="_blank">Yanqing Liu</a>,
      <a href="https://www.haqtu.me/" target="_blank">Haoqin Tu</a>,
      <a href="https://scholar.google.com/citations?user=G8NZJLIAAAAJ&hl=en" target="_blank">Hongru Zhu</a>,
      <a href="https://cihangxie.github.io/" target="_blank">Cihang Xie</a>.
    </p>
  </div>
</footer>


</body>
</html>

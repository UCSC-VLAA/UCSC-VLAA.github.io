<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>VLAA</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/UCSC_icon.png" rel="icon">
  <link href="assets/img/UCSC_icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Medilab - v4.7.1
  * Template URL: https://bootstrapmade.com/medilab-free-medical-bootstrap-theme/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Top Bar ======= -->
  <div id="topbar" class="d-flex align-items-center fixed-top">
    <div class="container d-flex justify-content-between">
      <div class="contact-info d-flex align-items-center">
      </div>
      <div class="d-none d-lg-flex social-links align-items-center">
        <a href="opening.html" class="envelope"><i class="bi-envelope"></i></a>
      </div>
    </div>
  </div>








  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center">

      <h1 class="logo me-auto"><a href="index.html">VLAA lab</a></h1>
      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="nav-link scrollto active" href="index.html">Home</a></li>
          <li><a class="nav-link scrollto" href="people.html">People</a></li>
          <li><a class="nav-link scrollto" href="publications.html">Publications</a></li>
          <li><a class="nav-link scrollto" href="https://github.com/UCSC-VLAA">GitHub</a></li>
          <li><a class="nav-link scrollto" href="https://huggingface.co/UCSC-VLAA">HuggingFace</a></li>
          <li><a class="nav-link scrollto" href="opening.html">Opening</a></li>


        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->







  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex align-items-center">
    <div class="container">
      <h1 >Welcome to VLAA lab</h1>

      <br>
      <h2>Vision · Learning  · Assured Autonomy</h2>
      <a href="publications.html" class="btn-get-started scrollto">Our projects</a>
    </div>
  </section><!-- End Hero -->

      <section class="inner-page">
      <div class="container">
        <p>
          <!-- Our lab is in the <a style=" color:#c85d44; ">Computer Science and Engineering Department, UC Santa Cruz.</a><br> -->
          <!-- With the goal of building human-level computer vision systems,<br> -->
          Our lab has a broad interest in computer vision and machine learning, including developing efficient deep representation learning with minimal supervision and securing model performance under (adversarial) distribution shifts. In addition, we are interested in 1) building transparent and trustworthy medical AI systems that can function effectively in complex clinical environments; and 2) generative AI, including LLMs and Diffusion Models.
      </div>
    </section>


  <main id="main">


      <div class="container-fluid" style="margin-bottom: -50px ">
             <div class="section-title">
                 <h2>News</h2>
<!--                  <p>The latest news of the UCSC VLAA lab.</p> -->
             </div>
      </div>

      <section class="inner-page">
      <div class="container">

          <font color="818589">
          <font size="4" color="#343434 ">[Q3&nbsp2025]</font>
          <li>MLRM-HALU is accepted by NeurIPS 2025.</li>
          <li>VLAA-Thinker is accepted by TMLR.</li>
          <li>ViLBench is accepted by EMNLP 2025.</li>
          <li>LVM-Lite is accepted by WACV 2026.</li>
          <li>Cihang Xie and Yuyin Zhou will serve as Area Chairs for CVPR 2026 and ICLR 2026.</li><br>


          <font color="818589">
          <font size="4" color="#343434 ">[Q2&nbsp2025]</font>
          <li>Three papers (OpenVision, VideoLLaMB, MedSegFactory) are accepted by ICCV 2025.</li>
          <li>Diff3M is accepted by MICCAI 2025.</li>
          <li>Two papers (Recap-DataComp-1B, PixelScaling) are accepted by ICML 2025.</li>
          <li>One papers (AQA-Bench) is accepted by TMLR.</li><br>

          <font color="818589">
          <font size="4" color="#343434 ">[Q1&nbsp2025]</font>
          <li>Three papers (LayerDecomp, Adventurer, Mamba®) are accepted by CVPR 2025.</li>
          <li>Cihang Xie will serve as Area Chairs for ICCV 2025 and NeurIPS 2025.</li>
          <li>Three papers (HQ-Edit, MedTrinity-25M, ARM) are accepted by ICLR 2025.</li>
          <li>Two papers (AttnGCG, ARVideo) are accepted by TMLR.</li><br>


          <font color="818589">
          <font size="4" color="#343434 ">[Q4&nbsp2024]</font>
          <li>We release <a href="https://ucsc-vlaa.github.io/CLIPS/">CLIPS</a>, a significantly enhanced CLIP model with SOTA cross-modal retrieval performance on MSCOCO and Flickr30K.</li>
          <li>We release <a href="https://jwmao1.github.io/storyadapter/">Story-Adapter</a>, a training-free and computationally efficient framework for enhancing the generative capability of long stories.</li>
          <li>One paper is accepted by AAAI.</li>
          <li>Two papers are accepted by TMLR.</li>
          <li>Cihang Xie will serve as Area Chairs for ICML 2025.</li><br>


          <font color="818589">
          <font size="4" color="#343434 ">[Q3&nbsp2024]</font>
          <li>We release <a href="https://yunfeixie233.github.io/MedTrinity-25M/">MedTrinity-25M</a>, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases.</li>
          <li>Three papers are accepted by NeurIPS 2024.</li>
          <li>Cihang Xie and Yuyin Zhou will serve as Area Chairs for CVPR 2025.</li><br>


          <font size="4" color="#343434 ">[Q2&nbsp2024]</font>
          <li>Three papers are accepted by ECCV 2024.</li>
          <li>We release <a href="https://www.haqtu.me/Recap-Datacomp-1B/">Recap-DataComp-1B</a>, where we use a LLaMA-3-powered LLaVA model to recaption the entire 1.3 billion images from DataComp-1B. Our Recap-DataComp-1B shows higher textual quality, and can help to train stronger CLIP models and T2I models.</li>
          <li><a href="https://github.com/OliverRensu/D-iGPT">D-iGPT</a> is accepted by ICML 2024. By scaling the model size to ViT-H, D-iGPT strongly secures an ImageNet top-1 accuracy of 90.0%.</li>
          <li>Congratulations to <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a> on winning the 2024-25 Jack Baskin & Peggy Downes-Baskin Fellowship.</li>
          <li>We release <a href="https://thefllood.github.io/HQEdit_web/">HQ-Edit</a>, a dataset with high-resolution images & detailed and aligned editing instructions. Our fine-tuned InstructPix2Pix delivers superior editing performance.</li>
          <li>Two papers are accepted by TMLR.</li><br>

          <font size="4" color="#343434 ">[Q1&nbsp2024]</font>
          <li>One paper is accepted by NAACL 2024.</li>
          <li>One paper is accepted by TMLR.</li>
          <li>Six papers are accepted by CVPR 2024.</li>
          <li>One paper has been accepted by ISBI 2024.</li>
          <li>Tuning LayerNorm in Attention is accepted by ICLR 2024 as a spotlight paper.</li>
          <li>Visual Probing is accepted by EACL 2024.</li><br>


          <font size="4" color="#343434 ">[Q4&nbsp2023]</font>
          <li>Congratulations to <a href="https://asillycat.github.io/">Zijun</a> and the team on winning the <strong>2nd</strong> place in both the base model subtrack and the large model subtrack, Red Teaming Track, in <a href="https://trojandetection.ai/">NeurIPS 2023 Trojan Detection Challenge</a></strong>. We will release the code and the report soon.</li>
          <li>We release <a href="https://github.com/OliverRensu/D-iGPT">D-iGPT</a>, which attains 89.5% ImageNet top-1 accuracy with ViT-L. Larger models are coming!</li>
          <li>One paper is accepted by TMLR.</li>
          <li>Cihang Xie will serve as Area Chairs for ICML 2024.</li>
          <li>Yuyin Zhou will serve as Area Chairs for CHIL 2024.</li>
          <li>CLIPA is accepted by NeurIPS 2023. In addition, we release our best model, <a href="https://github.com/UCSC-VLAA/CLIPA">CLIPA-G/14</a>, which attains 83.0% zero-shot ImageNet top-1 accuracy.</li>
          <li>CLIPA-v2 is accepted by NeurIPS 2023 R0-FoMo Workshop.</li>
          <li>Sight Beyond Text is accepted by NeurIPS 2023 Instruction Workshop.</li>
          <li>Congratulations to <a href="https://laos-y.github.io/">Siwei</a> and the team on winning the <strong>2nd</strong> place in <a href="https://www.synapse.org/#!Synapse:syn51156910/wiki/621282">Task 4: Brain Metastases Segmentation of MICCAI 2023 BraTS Challenge</a>.</li><br>

          <font size="4" color="#343434 ">[Q3&nbsp2023]</font>
          <li>Cihang Xie and Yuyin Zhou will serve as Area Chairs for ICLR 2024.</li>
          <li>Three papers (DiffMAE + SMAUG + BEVDistill) are accepted by ICCV 2023.</li><br>

          <font size="4" color="#343434 ">[Q2&nbsp2023]</font>
          <li>Two papers (SwinMM + MLB-Seg) are accepted by MICCAI 2023.</li>
          <li>Cihang Xie and Yuyin Zhou will serve as Area Chairs for CVPR 2024.</li>
          <li>We release <a href="https://github.com/UCSC-VLAA/CLIPA">CLIPA</a>, which enables CLIP training with limited computational resources. Furthermore, at a low cost of just $15,000, our CLIPA-v2 effectively elevates the zero-shot ImageNet top-1 accuracy to an impressive <strong>81.8%</strong>.</li><br>

          <font size="4" color="#343434 ">[Q1&nbsp2023]</font>
          <li>Yuyin Zhou receives the 2023 UCSC Hellman Fellowship.</li>
          <li>DMAE is accepted by CVPR 2023.</li>
          <li>Cihang Xie will serve as an Area Chair for NeurIPS 2023.</li>
          <li>Two papers (RobustCNN + One-Pixel Shortcut) are accepted by ICLR 2023.</li>
          <li>Yuyin Zhou will serve as an Area Chair for MICCAI 2023.</li><br>

          <font size="4" color="#343434 ">[Q4&nbsp2022]</font>
          <li>One paper (SSL-FL) is accepted by IEEE TMI.</li>
          <li>One paper (BNET) is accepted by IEEE TPAMI.</li>
          <li>Yuyin Zhou will serve as an Area Chair for CHIL 2023.</li>
          <li>Cihang Xie will serve as an Area Chair for ICML 2023.</li>
          <li>One paper (Practical Disruption of Deepfake) is accepted by AAAI 2023.</li>
          <li>Cihang Xie will be giving a talk in ECCV 2022 Workshop on Adversarial Robustness in the Real World.</li>                
          <li>Cihang Xie will serve as an Area Chair for CVPR 2023 and ICCV 2023.</li><br>


          <font size="4" color="#343434 ">[Q3&nbsp2022]</font>
          <li>Three papers (Adversarial Attack on Attackers + Counterfactual Simulation Testing + Multi-Granularity Cross-modal Alignment) are accepted by NeurIPS 2022.</li>
          <li>One paper (Across Feature Map Attention) is accepted by IEEE TPAMI.</li> 
          <li>Yuyin Zhou is selected as a finalist for MICCAI 2022 Young Scientist Publication Impact Award.</li>
          <li>Cihang Xie will serve as an Area Chair for ICLR 2023.</li>
          <li>Two papers (Efficient Video Pretraining with Images + ViP) are accepted by ECCV 2022.</li>
          <li>Two papers (CateNorm + MINiT) are accepted by MICCAI Workshop 2022.</li><br>


          <font size="4" color="#343434 ">[Q2&nbsp2022]</font>
          <li>Our CVPR 2022 MCV workshop, co-organized by Yuyin Zhou, is featured in CVPR Daily and Best of CVPR!</li>
          <li>Cihang Xie will be giving a talk in CVPR 2022 Workshop on The Art of Robustness: Devil and Angel in Adversarial Machine Learning.</li><br>


          <font size="4" color="#343434 ">[Q1&nbsp2022]</font>
          <li>Four papers (Simulated Adversarial Testing + SDMP + CD2-pFed + CNN vs. Transformer in Federated Learning) are accepted by CVPR 2022.</li>
          <li>Cihang Xie will serve as an Area Chair for NeurIPS 2022.</li>
          <li>Cihang Xie will be giving a talk in AAAI 2022 1st International Workshop on Practical Deep Learning in the Wild.</li>
          <li>Cihang Xie will serve as an Area Chair for ECCV 2022.</li>
           <li>Two papers (Fast AdvProp + iBOT) are accepted by ICLR 2022.</li>
           <li>Yuyin Zhou will serve as an Area Chair for MICCAI 2022.</li><br>


          <font size="4" color="#343434 ">[Q4&nbsp2021]</font>
          <li>One paper (External Attention Assisted Training) is accepted by IEEE TMI.</li>
           <li>Cihang Xie will be giving a talk in NeurIPS 2021 Visual Domain Adaptation Challenge.</li>
           <li>Yuyin Zhou will serve as an Area Chair for CHIL 2022.</li>
           <li>Cihang Xie will be giving a talk in ICCV 2021 2nd Workshop on Adversarial Robustness In the Real World.</li>
           <li>Our ICCV Computer Vision for Automated Medical Diagnosis Workshop, co-organized by Yuyin Zhou, is featured in ICCV Daily!</li><br>

           <font size="4" color="#343434 ">[Q3&nbsp2021]</font>
           <li>One paper (CNN vs. Transformer) is accepted by NeurIPS 2021.</li>
           <li>One paper (CaliCO) is accepted by ICCV 2021.</li>
           <li>Cihang Xie will be giving a talk in ICML 2021 Workshop on A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning.</li><br>


           <font size="4" color="#343434 ">[Q2&nbsp2021]</font>
           <li>One paper (Phenotype Embedding Model) is accepted by MICCAI 2021.</li>
           <li>Yuyin Zhou will be giving a talk in CVPR 2021 Workshop on Medical Computer Vision.</li>
           <li>Cihang Xie will be giving a talk in CVPR 2021 Tutorial on Adversarial Machine Learning in Computer Vision.</li>
           <li>Cihang Xie will serve as an Area Chair for ICLR 2022.</li><br>


           <font size="4" color="#343434 ">[Q1&nbsp2021]</font>
           <li>One paper (DetAdvProp) is accepted by CVPR 2021.</li>
           <li>One paper (IAG-Net) is accepted by IEEE TMI.</li>
           <li>One paper (Shape-Texture Debiased Training) is accepted by ICLR 2021.</li>
           <li>Cihang Xie will serve as an Area Chair for ICCV 2021.</li><br>

           </font>
      </div>
    </section>




  </main><!-- End #main -->


  <!-- ======= Footer ======= -->
  <footer id="footer">

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="copyright">
          &copy; Copyright <strong><span>VLAA</span></strong>. All Rights Reserved
        </div>
        <div class="credits">

          Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
        </div>
      </div>
                <div class="col-lg-4 col-md-6 footer-newsletter">
          </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>


